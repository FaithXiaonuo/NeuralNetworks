{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Dense_Layer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FaithXiaonuo/NeuralNetworks/blob/Neural_Networks/Dense_Layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhaScIdHJWWb"
      },
      "source": [
        "# **Instructions**\n",
        "For this task, we need to build our own fully connected neural network without the help of additional python packages (such as numpy).\n",
        "There will be two parts. First, we build a Keras model to train the MNIST data set and save the parameters. Second, we use our own neural network to rebuild the neural network based on the saved parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__mtDwITG9UQ"
      },
      "source": [
        "# **part 1**\n",
        "In this part, we will use the keras model to build a neural network. Train the model so that the accuracy of training and verification can reach more than 97% and 93%. Save the parameters, which will be used in Part 2, including weights and biases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgzVKUea9taF"
      },
      "source": [
        "# Import TensorFlow into your program:\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Flatten, ReLU\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "from keras.datasets import mnist\n",
        "\n",
        "from google.colab import files\n",
        "import h5py"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FR9F1GHR9tid"
      },
      "source": [
        "# Load and prepare the MNIST dataset.\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Add a channels dimension\n",
        "x_train = x_train[..., tf.newaxis].astype(\"float32\")\n",
        "x_test = x_test[..., tf.newaxis].astype(\"float32\")\n",
        "\n",
        "# Use tf.data to batch and shuffle the dataset:\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Zi8ug6X9tq0"
      },
      "source": [
        "# Build the tf.keras model using the Keras model subclassing API:\n",
        "class MyModel(Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.flatten = Flatten()\n",
        "    self.d1 = Dense(28, activation='relu')\n",
        "    self.d2 = Dense(10)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    x = self.d1(x)\n",
        "    return self.d2(x)\n",
        "\n",
        "# Create an instance of the model\n",
        "model = MyModel()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0YUDez-9tzM"
      },
      "source": [
        "# Choose an optimizer and loss function for training:\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# Select metrics to measure the loss and the accuracy of the model. \n",
        "# These metrics accumulate the values over epochs and then print the overall result.\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD76gIvv_kb3"
      },
      "source": [
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # training=True is only needed if there are layers with different\n",
        "        # behavior during training versus inference (e.g. Dropout).\n",
        "        predictions = model(images, training=True)\n",
        "        loss = loss_object(labels, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(labels, predictions)\n",
        "\n",
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "    # training=False is only needed if there are layers with different\n",
        "    # behavior during training versus inference (e.g. Dropout).\n",
        "    predictions = model(images, training=False)\n",
        "    t_loss = loss_object(labels, predictions)\n",
        "\n",
        "    test_loss(t_loss)\n",
        "    test_accuracy(labels, predictions)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFgDeiFs_kj3",
        "outputId": "cb9e95a0-5a83-46e7-99ac-03c52457e184"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # Reset the metrics at the start of the next epoch\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "\n",
        "    for images, labels in train_ds:\n",
        "        train_step(images, labels)\n",
        "\n",
        "    for test_images, test_labels in test_ds:\n",
        "        test_step(test_images, test_labels)\n",
        "\n",
        "    print(\n",
        "      f'Epoch {epoch + 1}, '\n",
        "      f'Loss: {train_loss.result()}, '\n",
        "      f'Accuracy: {train_accuracy.result() * 100}, '\n",
        "     f'Test Loss: {test_loss.result()}, '\n",
        "      f'Test Accuracy: {test_accuracy.result() * 100}'\n",
        "    )"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.382997065782547, Accuracy: 89.31666564941406, Test Loss: 0.2404157519340515, Test Accuracy: 92.91999816894531\n",
            "Epoch 2, Loss: 0.21424515545368195, Accuracy: 93.80833435058594, Test Loss: 0.189997136592865, Test Accuracy: 94.2300033569336\n",
            "Epoch 3, Loss: 0.1723167598247528, Accuracy: 95.0250015258789, Test Loss: 0.1650402694940567, Test Accuracy: 95.04000091552734\n",
            "Epoch 4, Loss: 0.14684751629829407, Accuracy: 95.75499725341797, Test Loss: 0.1508215367794037, Test Accuracy: 95.51000213623047\n",
            "Epoch 5, Loss: 0.12937599420547485, Accuracy: 96.24333190917969, Test Loss: 0.14183661341667175, Test Accuracy: 95.6300048828125\n",
            "Epoch 6, Loss: 0.11640485376119614, Accuracy: 96.61166381835938, Test Loss: 0.13181443512439728, Test Accuracy: 95.92000579833984\n",
            "Epoch 7, Loss: 0.10591881722211838, Accuracy: 96.95166778564453, Test Loss: 0.13015875220298767, Test Accuracy: 95.9000015258789\n",
            "Epoch 8, Loss: 0.09730491042137146, Accuracy: 97.19667053222656, Test Loss: 0.12444835901260376, Test Accuracy: 96.10000610351562\n",
            "Epoch 9, Loss: 0.09008577466011047, Accuracy: 97.3983383178711, Test Loss: 0.12353821843862534, Test Accuracy: 96.30000305175781\n",
            "Epoch 10, Loss: 0.08370506763458252, Accuracy: 97.5683364868164, Test Loss: 0.12176354974508286, Test Accuracy: 96.3800048828125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TlCNMH__kof"
      },
      "source": [
        "# Save the weights\n",
        "model.save_weights('/content/my_model.h5')\n",
        "# files.download('/content/my_model.h5')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hQxUDg9_ksf",
        "outputId": "51b5b055-eca9-4695-862e-db6ed0bb34ca"
      },
      "source": [
        "# Restore the weights\n",
        "model2 = MyModel()\n",
        "model2.build((32, 28, 28, 1))\n",
        "model2.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss_object,\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "model2.load_weights('/content/my_model.h5')\n",
        "model2.evaluate(x_train, y_train)\n",
        "model2.evaluate(x_test, y_test)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0761 - accuracy: 0.9764\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1219 - accuracy: 0.9638\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.12194637209177017, 0.9638000130653381]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4FsH2AEIxMW"
      },
      "source": [
        "# **Part 2**\n",
        "In this part, we use our own neural network to reconstruct the neural network based on the parameter file. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVneR0Az_6gv"
      },
      "source": [
        "# load weights data from .h5 file\n",
        "load_weights = h5py.File('/content/my_model.h5')\n",
        "# convert weights and biases into lists\n",
        "load_w_d0 = load_weights['dense']['my_model']['dense']['kernel:0'][:]\n",
        "load_w_d1 = load_weights['dense_1']['my_model']['dense_1']['kernel:0'][:]\n",
        "\n",
        "load_b_d0 = load_weights['dense']['my_model']['dense']['bias:0'][:]\n",
        "load_b_d1 = load_weights['dense_1']['my_model']['dense_1']['bias:0'][:]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zAzd_EgQQci"
      },
      "source": [
        "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
        "train_x, test_x = train_x / 255.0, test_x / 255.0\n",
        "train_x = train_x[..., tf.newaxis].astype(\"float32\")\n",
        "test_x = test_x[..., tf.newaxis].astype(\"float32\")\n",
        "train_x = [train_x[i:i+32] for i in range(int(len(train_x) / 32))]\n",
        "train_y = [train_y[i:i+32] for i in range(int(len(train_y) / 32))]\n",
        "test_x = [test_x[i:i+32] for i in range(int(len(test_x) / 32))]\n",
        "test_y = [test_y[i:i+32] for i in range(int(len(test_y) / 32))]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Tjngg-8_6kf"
      },
      "source": [
        "# reimplemented the layer function, including flatten, ReLU, Math of Matrix(Dot, Plus)\n",
        "def flatten(data):\n",
        "    output = [[[item for sublist in batch for filter in sublist for item in filter]] for batch in data]\n",
        "    return output"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMYEzHYJ_6no"
      },
      "source": [
        "def myReLUforDense(input):\n",
        "    input = [[[0 if input[i][0][k] < 0 else input[i][0][k] for k in range(len(input[i][0]))]] for i in range(len(input))]\n",
        "    return input"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7PKcWH5QtNX"
      },
      "source": [
        "def myMax(input):\n",
        "    output = []\n",
        "    for i in range(len(input)):\n",
        "        max = 0\n",
        "        for j in range(1, len(input[i][0])):\n",
        "            if input[i][0][max] < input[i][0][j]:\n",
        "                max = j\n",
        "        output.append(max)\n",
        "    return output"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zE4EsViMlskq"
      },
      "source": [
        "def myDot(MA, MB):\n",
        "    output = [[sum(a * b for a, b in zip(A_row, B_col)) \n",
        "                              for B_col in zip(*MB)]\n",
        "                              for A_row in MA]\n",
        "    return output\n",
        "\n",
        "def myPlus(A, B):\n",
        "    output = [[A[0][i] + B[i] for i in range(len(A[0]))]]\n",
        "    return output"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7zd44_L_6rO"
      },
      "source": [
        "# my reimplemented Dense \n",
        "class Dense():\n",
        "    def __init__(self, weights, biases):\n",
        "        super(Dense, self).__init__()\n",
        "        self.weights = weights\n",
        "        self.biases = biases\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        return [myPlus(myDot(input, self.weights), self.biases) for input in inputs]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9piN9TVAIl-"
      },
      "source": [
        "# my rewrite model\n",
        "def myNewModel(input, dense0, dense1):\n",
        "    input = flatten(input)\n",
        "    input = dense0.forward(input)\n",
        "    input = myReLUforDense(input)\n",
        "    input = dense1.forward(input)\n",
        "    input = myMax(input)\n",
        "    return input"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "R6Tr2o72ANOX",
        "outputId": "02a3f006-619d-4531-cccb-2197021d18f7"
      },
      "source": [
        "acc_train = 0\n",
        "lgh_train = 0\n",
        "acc_test = 0\n",
        "lgh_test = 0\n",
        "dense0 = Dense(load_w_d0, load_b_d0)\n",
        "dense1 = Dense(load_w_d1, load_b_d1)\n",
        "\n",
        "for i in range(len(train_y)): \n",
        "    output = []\n",
        "    diff = 0\n",
        "    out = myNewModel(train_x[i], dense0, dense1)\n",
        "    for index in range(len(out)):\n",
        "        if out[index] != train_y[i][index]:\n",
        "            diff += 1\n",
        "    acc = 1 - diff/len(labels)\n",
        "    acc_train += acc\n",
        "    lgh_train += 1\n",
        "\n",
        "for i in range(len(test_y)): \n",
        "    output = []\n",
        "    diff = 0\n",
        "    out = myNewModel(test_x[i], dense0, dense1)\n",
        "    for index in range(len(out)):\n",
        "        if out[index] != test_y[i][index]:\n",
        "            diff += 1\n",
        "    acc = 1 - diff/len(labels)\n",
        "    acc_test += acc\n",
        "    lgh_test += 1\n",
        "\n",
        "print(\"train_accuracy:\" + str(acc_train/lgh_train * 100) + \". test_accuracy:\" + str(acc_test/lgh_test * 100))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch: 20 acc: 1.0\n",
            "batch: 40 acc: 1.0\n",
            "batch: 60 acc: 1.0\n",
            "batch: 80 acc: 1.0\n",
            "batch: 100 acc: 1.0\n",
            "batch: 120 acc: 0.9375\n",
            "batch: 140 acc: 0.9375\n",
            "batch: 160 acc: 1.0\n",
            "batch: 180 acc: 1.0\n",
            "batch: 200 acc: 0.96875\n",
            "batch: 220 acc: 0.9375\n",
            "batch: 240 acc: 0.9375\n",
            "batch: 260 acc: 0.9375\n",
            "batch: 280 acc: 0.96875\n",
            "batch: 300 acc: 0.96875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-25ed7b0906d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlgh_test\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'batch: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlgh_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' acc: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_accuracy:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_train\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlgh_train\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\". test_accuracy:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_test\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlgh_test\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ]
    }
  ]
}